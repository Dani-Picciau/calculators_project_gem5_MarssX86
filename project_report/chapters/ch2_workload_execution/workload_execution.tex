\documentclass[../../main_document/main.tex]{subfiles}
\externaldocument{../../main_document/main}

\begin{document}
\section{Esecuzione di un workload tramite i simulatori}
L'obiettivo di questo progetto è analizzare e confrontare le prestazioni di tre diverse architetture (ISA): ARM, RISC-V e x86, utilizzando un workload computazionale standardizzato.
Per poter eseguire questo confronto fra le differenti architetture è imperativo stabilire una metodologia che ne  garantisca l'equità.

\subsection{Definizione di "piccolo workload"}
Le istruzioni del progetto richiedono l'esecuzione di un "piccolo workload" con tracing e statistiche. Per garantire la riproducibilità e la confrontabilità tra ARM, RISC-V e x86, il workload non deve dipendere da librerie di sistema complesse che potrebbero variare tra le architetture.

\vspace{8pt}
\noindent
Il candidato ideale per questo workload è un algoritmo di moltiplicazione di matrici (Matrix Multiplication) denso. Questo kernel computazionale offre diversi vantaggi analitici:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Prevedibilità}: l'accesso alla memoria è regolare, permettendo di analizzare l'efficacia delle gerarchie di cache.
    \item \textbf{Intensità Computazionale}: stressa le unità funzionali della CPU e la logica di pipelining, evidenziando le differenze tra l'approccio RISC (molte istruzioni semplici) e CISC (istruzioni complesse con accessi in memoria impliciti).
    \item \textbf{Portabilità}: può essere scritto in C standard (ANSI C) e compilato staticamente per tutte e tre le architetture senza modifiche al codice sorgente.
\end{itemize}

\vspace{8pt}
\noindent
Il codice sorgente C proposto per l'esperimento dovrà essere compilato in tre binari distinti utilizzando cross-compilatori specifici per ARM, RISC-V e x86, come dettagliato nelle sezioni successive, ed è il seguente:
\begin{lstlisting}[style=mystyle, language=C, escapeinside={(*@}{@*)}]
#include <stdio.h>
#include <stdlib.h>

#define N 64 // Dimensione ridotta per simulazione rapida

void matrix_multiply(double *A, double *B, double *C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            double sum = 0.0;
            for (int k = 0; k < size; k++) {
                sum += A[i * size + k] * B[k * size + j];
            }
            C[i * size + j] = sum;
        }
    }
}

int main() {
    size_t bytes = N * N * sizeof(double);
    double *A = (double *)malloc(bytes);
    double *B = (double *)malloc(bytes);
    double *C = (double *)malloc(bytes);

    // Inizializzazione deterministica
    for (int i = 0; i < N * N; i++) {
        A[i] = 1.0;
        B[i] = 2.0;
    }

    printf("Inizio Benchmark Moltiplicazione Matrici %dx%d\n", N, N);
    matrix_multiply(A, B, C, N);
    printf("Fine Benchmark\n");

    return 0;
}                                           
\end{lstlisting}

\subsection{Problema metodologico}
Una delle sfide principali di questo confronto risiede nella natura diversa degli ambienti di simulazione disponibili:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Gem5} offre la modalità \textbf{Systemcall Emulation (SE)} la quale non simula un vero computer con un disco rigido e un sistema operativo completo. Permette di simulare l'esecuzione di binari user-space evitando di modellare i dispositivi o un OS, emulando la maggior parte dei servizi a livello di sistema;
    \item \textbf{MarssX86} operà in modalità \textbf{Full System (FS)}, simulando un'intera macchina fisica, incluso il sistema operativo guest, i device e gli interrupt.
\end{itemize}

\subsubsection{Il caso Gem5 in modalità (SE)}
Nella compilazione standard (dinamica), l'eseguibile generato è incompleto: esso contiene riferimenti a funzioni esterne (come printf o malloc presenti nella glibc) ma non il loro codice macchina. Al momento dell'avvio, prima ancora che venga eseguita la funzione main(), il sistema operativo invoca il Dynamic Linker, il quale deve:
\begin{itemize}[leftmargin=1em]
    \item Esplorare il filesystem alla ricerca delle librerie condivise (.so) richieste;
    \item Caricarle nella memoria virtuale del processo;
    \item Risolvere i simboli, collegando le chiamate del programma agli indirizzi di memoria delle librerie appena caricate;
\end{itemize}

\vspace{8pt}
\noindent
Proprio per questo motivo, questo \textbf{meccanismo risulta critico} in modalità SE, poiché:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Assenza di Filesystem Virtuale Completo}: Gem5 in modalità SE non dispone di un filesystem guest nativo. Quando il Dynamic Linker (che è un programma a sé stante) cerca le librerie in percorsi standard (es. /lib/ o /usr/lib/), tali percorsi potrebbero non esistere nell'ambiente simulato o non coincidere con i percorsi delle librerie cross-compilate presenti sulla macchina host.
    \item \textbf{Complessità delle Syscall}: Il Dynamic Linker fa un uso intensivo di chiamate di sistema per mappare file in memoria (open, mmap). Sebbene Gem5 emuli le syscall comuni (come read()), la gestione complessa del caricamento dinamico può portare a errori di path lookup o incompatibilità di ABI (Application Binary Interface) tra host e guest.
\end{itemize}
\noindent
Per oviare a queste problematiche e garantire la riproducibilità degli esperimenti si rende dunque necessario imporre dei vincoli specifici sulla fase di compilazione e linking del workload che viene compilato staticamente utilizzando il flag \texttt{-static}.\\
Con il linking statico, tutte le dipendenze necessarie (inclusa la libc) vengono incorporate fisicamente all'interno del file binario al momento della compilazione. Ciò rende l'eseguibile autosufficiente:
\begin{itemize}[leftmargin=1em]
    \item Elimina la necessità del Dynamic Linker in fase di avvio;
    \item Rimuove la dipendenza da librerie esterne durante la simulazione;
    \item Permette a Gem5 di caricare il binario in memoria ed eseguire direttamente il codice utente, intercettando ed emulando puntualmente le sole System Call esplicite generate dal programma (es. output su terminale), garantendo stabilità e portabilità tra diversi ambienti host.
\end{itemize}

\subsubsection{Il caso MarssX86 (FS)}
Nonostante il simulatore MarssX86 sia adatto a far girare un vero sistema operativo, con disco virtuale, un kernel Linux vero e delle librerie installate dentro l'immagine del disco (quindi in modalità Full System), rimane un problema di \textbf{compatibilità delle versioni}, dovuto principalmente al fatto che si tratta di un simulatore datato.

\vspace{8pt}
\noindent
L'ambiente docker che viene utilizzato fornisce Ubuntu $18.04$, uscito nel $2018$, e utilizza la libreria glibc versione $2.27$. Invece, le immagini del disco che si devono utilizzare per il corretto funzionament su MarssX86 sono solitamente basate su ubuntu $10.04$ o $12.04$ (relative agli anni $2010-2012$) e utilizzano una libreria glibc versione $2.15$ o inferiore.\\
Purtroppo, come specificato al \textbf{\textit{"passo 2"}} del capitolo \ref{par:ch_1.3.1}, tali versioni dell'ambiente ubuntu sono state eliminate dai server.\\
La libreria standard di C non garantisce una \textit{forward compatibility} (compatibilità in avanti):
\begin{itemize}[leftmargin=1em]
    \item Un programma compilato su un sistema vecchio gira su uno nuovo (Backward Compatibility);
    \item Un programma compilato su un sistema nuovo (il Docker con 18.04) NON gira su uno vecchio (MarssX86).
\end{itemize}

\vspace{8pt}
\noindent
Poiché l'ambiente di compilazione (Host) dispone di librerie di sistema (libc) molto più recenti rispetto all'ambiente simulato (Guest OS nell'immagine disco), un linking dinamico causerebbe errori di runtime dovuti al version mismatch delle librerie condivise.\\
Una possibile soluzione è utilizzare anche in questo caso il linking statico, che disaccoppia il benchmark dalle librerie del sistema guest.
Pertanto, l'unico modo per riuscire a far partire il compilatore MarssX86 è utilizzare l'opzione \texttt{-static} rendendo il programma indipendente dalla "vecchiaia" del sistema operativo simulato dentro MarssX86.


\subsection{Strategia adottata}
Per garantire un confronto il più equo possibile nonostante queste differenze strutturali, è stata adottata la seguente metodologia:
\begin{enumerate}[leftmargin=1em]
    \item \textbf{Isolamento Microarchitetturale (Gem5)}: Per le architetture ARM e RISC-V su Gem5, abbiamo scelto la modalità \textbf{SE}. Questa scelta ci permette di misurare le prestazioni pure della CPU e della gerarchia di memoria (cache), eliminando il 'rumore' introdotto dai servizi del kernel e dallo scheduling dei processi;
    \item \textbf{Realismo Operativo (MarssX86)}: Per l'architettura x86 su MarssX86, operiamo in un contesto FS. Siamo consapevoli che i risultati includeranno l'overhead del sistema operativo (context switch, gestione TLB software);
    \item \textbf{Mitigazione tramite Workload e Compilazione}:
    \begin{itemize}[leftmargin=1em, label=\raisebox{0.4ex}{\phantom{1}\rule{0.6ex}{0.6ex}}]
        \item Abbiamo scelto un workload CPU-bound (moltiplicazione matrici) e non I/O-bound. Questo minimizza le chiamate di sistema, rendendo le prestazioni dipendenti quasi esclusivamente dalla potenza di calcolo della CPU e dall'efficienza delle cache, riducendo il divario tra modalità SE e FS;
        \item Abbiamo utilizzato la compilazione statica (\texttt{-static}) per tutti i binari. Questo garantisce che su Gem5 (SE) il simulatore possa emulare le syscall senza dipendenze esterne, e su MarssX86 (FS) evita conflitti di librerie (ABI mismatch) tra la macchina host e il sistema guest simulato.
    \end{itemize}
\end{enumerate}

\subsection{Metodologia e Configurazione degli Esperimenti}
Una volta stabilita la natura del workload e le modalità di simulazione, la fase sperimentale prevede l'esecuzione di una matrice di test strutturata per isolare l'impatto delle diverse configurazioni microarchitetturali.

\subsubsection{Matrice dei test e definizione delle Baseline}
Per ciascuna delle tre architetture analizzate (ARM, RISC-V e x86), verranno eseguite tre diverse configurazioni di simulazione, per un totale di 9 test. L'obiettivo è isolare l'impatto del modello di esecuzione della CPU e della gerarchia di memoria sulle prestazioni del workload.


\vspace{8pt}
\noindent
Le varianti si basano su due modelli di processore fondamentali:
\begin{itemize}[leftmargin=1em]
    \item \textbf{\textit{In-Order}}: un modello in cui le istruzioni vengono eseguite nell'esatto ordine in cui appaiono nel programma. È un'architettura più semplice ed efficiente dal punto di vista energetico, ma soggetta a "stalli" (blocchi) se un'istruzione deve attendere dati dalla memoria.
    \item \textbf{\textit{Out-of-Order (OoO)}}: un modello avanzato che permette alla CPU di eseguire le istruzioni non appena le loro dipendenze sono soddisfatte, anche se non in ordine sequenziale. Questo permette di "nascondere" le latenze della memoria, ma richiede una logica hardware molto più complessa (scheduler, registri di rinomina, ecc.).
\end{itemize}

\vspace{8pt}
\noindent
Per ogni architettura sono state definite le seguenti tre configurazioni:
\begin{enumerate}[leftmargin=1.3em]
    \item \textbf{Configurazione 1 (Baseline)}: Utilizza una CPU \textbf{Out-of-Order} ad alte prestazioni. La gerarchia di memoria prevede una cache L1 da 128\,KB e una cache L2 da 2\,MB. Rappresenta il target di massima potenza computazionale.
    \item \textbf{Configurazione 2 (CPU Efficiency Test)}: Mantiene le stesse cache della baseline (L1 128\,KB, L2 2\,MB) ma sostituisce il modello di core con una CPU \textbf{In-Order}. Questo test serve a misurare quanto l'architettura tragga beneficio dall'esecuzione fuori ordine per il particolare workload di moltiplicazione matriciale.
    \item \textbf{Configurazione 3 (Memory Constraint Test)}: Ritorna al modello di CPU \textbf{Out-of-Order} della baseline, ma riduce drasticamente le dimensioni delle cache (L1 a 32\,KB e L2 a 256\,KB). Questo permette di osservare quanto le prestazioni siano limitate dalla capacità della memoria (memory-bound) rispetto alla potenza pura di calcolo.
\end{enumerate}

\subsection{Esecuzione del workload per ARM e RISC-V (Gem5)}
Per poter eseguire il workload scelto sulle architetture target (ARM e RISC-V), è prima necessario affrontare il problema della compatibilità binaria. Poiché le macchine utilizzate per lo sviluppo (Host) dispongono solitamente di un set di istruzioni x86\_64 o ARM64 (Apple Silicon), i binari prodotti da un compilatore standard non sarebbero comprensibili dai simulatori configurati per architetture diverse.

\vspace{8pt}
\noindent
In particolare, anche qualora si utilizzasse un processore Apple Silicon (già basato su ARM), sarebbe comunque necessaria la cross-compilazione (o più precisamente una compilazione specifica per il Guest) principalmente per due motivi: 
\begin{itemize}[leftmargin=1em] 
    \item \textbf{ABI (Application Binary Interface)}: il binario "nativo" di un Mac è progettato per macOS (formato Mach-O), mentre Gem5 richiede un binario in formato Linux ELF; 
    \item \textbf{Librerie e System Call}: macOS utilizza librerie di sistema Apple, mentre il simulatore Gem5 emula l'interfaccia delle chiamate di sistema Linux. 
\end{itemize}

\subsubsection{Installazione dei cross-compilatori su Windows (x86\_64)}
\label{par: ch_2.5.1}
Dunque, per prima cosa dobbiamo assicurarci di installare i "binari" giusti per la cross-compilazione. Nel sottosistema Ubuntu WSL, posizionati in \texttt{/home/NomeUtente/dev\_local/\ gem5}, utilizzare i seguenti comandi, uno alla volta:
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
sudo apt-get update
\end{lstlisting}  

\vspace{8pt}
\noindent
\textbf{\textit{Per l'architettura ARM}:}
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
sudo apt-get -y install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu
\end{lstlisting} 
Per verificare la corretta installazione del cross-compilatore lanciare il seguente comando:
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
aarch64-linux-gnu-gcc --version
\end{lstlisting} 
Infine, per compilare il file C si utilizza il seguente comando (ricordando \texttt{-static}):
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
aarch64-linux-gnu-gcc -static matrix_mul.c -o matrix_mul_arm
\end{lstlisting} 

\vspace{8pt}
\noindent
\textbf{\textit{Per l'architettura RISC-V}:} 
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
sudo apt-get -y install gcc-riscv64-linux-gnu g++-riscv64-linux-gnu
\end{lstlisting} 
Per verificare la corretta installazione del cross-compilatore lanciare il seguente comando:
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
riscv64-linux-gnu-gcc --version
\end{lstlisting} 
Infine, per compilare il file C si utilizza il seguente comando (ricordando \texttt{-static}):
\begin{lstlisting}[style=mystyle, escapeinside={(*@}{@*)}]
riscv64-linux-gnu-gcc -static matrix_mul.c -o matrix_mul_riscv
\end{lstlisting} 

\subsubsection{Installazione dei cross-compilatori su macOS (ARM)}
I comandi rimangono identici a quelli utilizzati su x86\_64 tranne che per qualche piccolo accorgimento.
Nel capitolo \ref{par:ch_1.2.2} è stato utilizzato docker per l'installazione del simulatore.\\
In questo caso, i comandi andranno utilizzati dalla seguente posizione \texttt{/root/gem5}:
\begin{lstlisting}[style=mystyle]
root@{...}:/gem5#
\end{lstlisting}
Inoltre, gli stessi, devono essere utilizzati senza il comando \textbf{"sudo"} davanti poiché una volta entrati nell'ambiente docekr si è già un amministratore.

\subsubsection{Comandi per la build del workload}
Il comando per la build del workload (il file C) rimane invariato rispetto al calcolatore in cui viene utilizzato.\\ Ogni comando di build, all'interno di \texttt{gem5/m5out/NomeCartella}, creerà un file \texttt{.txt} che conterrà tutte le statistiche relative alla simulazione con le caratteristiche specificate.\\
Dunque, sia che ci si trovi in \texttt{root/gem5} per macOS o in \texttt{/home/NomeUtente/\ dev\_local/gem5} su un sistema Windows i comandi saranno i seguenti:

\vspace{8pt}
\noindent
\textbf{\textit{Le tre build per l'architettura ARM}}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 1 (baseline)
./build/ARM/gem5.opt --outdir=m5out/arm_baseline configs/deprecated/example/se.py \
  --cpu-type=ArmO3CPU \
  --caches --l2cache \
  --l1d_size=128kB --l1i_size=128kB \
  --l2_size=2MB \
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_arm
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 2 (CPU efficiency test) 
./build/ARM/gem5.opt --outdir=m5out/arm_var1_cpu configs/deprecated/example/se.py \
  --cpu-type=ArmMinorCPU \
  --caches --l2cache \
  --l1d_size=12kB --l1i_size=128kB \
  --l2_size=2MB \
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_arm
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 3 (Memory costraint test)
./build/ARM/gem5.opt --outdir=m5out/arm_small_cache configs/deprecated/example/se.py \
  --cpu-type=ArmO3CPU \
  --caches --l2cache \
  --l1d_size=32kB --l1i_size=32kB \
  --l2_size=256kB \
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_arm
\end{lstlisting}

\vspace{8pt}
\noindent
\textbf{\textit{Le tre build per l'architettura RISC-V}} 
\begin{lstlisting}[style=mystyle, language= C]  
// Comando di build per la configurazione 1 (baseline)
./build/RISCV/gem5.opt --outdir=m5out/riscv_baseline configs/deprecated/example/se.py \
  --cpu-type=O3CPU \
  --caches --l2cache \
  --l1d_size=128kB --l1i_size=128kB \
  --l2_size=256kB\
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_riscv
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 2 (CPU efficiency test) 
./build/RISCV/gem5.opt --outdir=m5out/riscv_var1_cpu configs/deprecated/example/se.py \
  --cpu-type=MinorCPU \
  --caches --l2cache \
  --l1d_size=128kB --l1i_size=128kB \
  --l2_size=256kB  \
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_riscv
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 3 (Memory costraint test)
./build/RISCV/gem5.opt --outdir=m5out/riscv_small_cache configs/deprecated/example/se.py \
  --cpu-type=O3CPU \
  --caches --l2cache \
  --l1d_size=32kB --l1i_size=32kB \
  --l2_size=256kB \
  --sys-clock=2.688GHz \
  --cmd=./matrix_mul_riscv
\end{lstlisting}

\vspace{8pt}
\noindent
\textbf{\textit{Nota}:} cambiare O3CPU con ArmO3CPU ha senso per una questione di rigore metodologico e stabilità della simulazione per i seguenti motivi:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Specificità del modello}: Mentre O3CPU è un'etichetta generica, ArmO3CPU forza il simulatore a usare parametri di pipeline 
	(come la profondità degli stadi e la dimensione del Reorder Buffer) specificamente modellati sulle caratteristiche delle CPU ARM;
    \item \textbf{Versioni future}: Il file se.py è considerato "deprecated". 
	Nelle versioni più recenti di Gem5, l'automazione che associa O3CPU ad ArmO3CPU potrebbe essere rimossa, 
	rendendo il comando generico non funzionante.
\end{itemize}

\vspace{8pt}
\noindent
Questa cosa non vale in RISC-V dove il binario “build/RISCV/gem5.opt” contiene solo l’implementazione RISC-V quindi O3CPU non può essere altro che un processore
RISC-V.
 
\subsection{Esecuzione del workload per X86 (MarssX86)}
A causa dell'architettura di MarssX86, che si basa su una versione datata di QEMU, l'interazione standard con il simulatore presenta significative limitazioni operative per l'esecuzione di benchmark automatizzati.

\vspace{8pt}
\noindent
Inizialmente, la procedura standard prevedeva un intervento manuale dell'utente: era necessario avviare il workload nel sistema guest e, simultaneamente, inviare un comando al monitor di QEMU (tramite una combinazione di tasti rapida) per attivare la registrazione delle statistiche (simconfig -run). Questo approccio presentava due criticità fondamentali:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Mancanza di Determinismo}: Il tempo di reazione umano nell'impartire il comando di avvio non è costante. Questo rendeva impossibile sincronizzare perfettamente l'inizio della registrazione con l'inizio del calcolo matematico;
    \item \textbf{Inquinamento dei Dati (\textit{"Noise"})}: Per dare all'utente il tempo fisico di attivare il simulatore, era necessario inserire nel codice C delle funzioni di ritardo (come sleep()) o loop a vuoto prima del calcolo. Tuttavia, il simulatore, essendo cycle-accurate, registrava anche questi cicli di attesa (fino a 160 milioni di cicli inutili nei test preliminari), falsando drasticamente metriche chiave come l'IPC (Instructions Per Cycle) e il conteggio totale delle istruzioni.
\end{itemize}

\vspace{8pt}
\noindent
Per ovviare a queste problematiche e garantire la rigorosità scientifica dei dati, è stato necessario implementare una metodologia basata sulla \textbf{Region of Interest (ROI)}.\\
Questa tecnica sposta il controllo della simulazione dall'utente direttamente al codice sorgente.\\ 
Tramite l'inclusione della libreria specifica ptlcalls.h e l'uso di "hypercalls" (istruzioni speciali che comunicano direttamente con l'hardware simulato), è il benchmark stesso a ordinare al simulatore di accendersi (ptlcall\_switch\_to\_sim) esattamente un istante prima dell'inizio dell'algoritmo di moltiplicazione e di spegnersi (ptlcall\_switch\_to\_native) appena terminato.

\vspace{8pt}
\noindent
In questo modo, abbiamo eliminato completamente l'errore umano e il rumore di fondo, ottenendo misurazioni che riflettono esclusivamente il carico di lavoro computazionale oggetto di studio.

\subsubsection{Implementazione della metodologia ROI}
\label{par: ch_2.6.1}
Dal momento in cui sta venendo utilizzato docker per entrambi i sistemi macOS e Windows, la procedura da eseguire rimane invariata. Possiamo suddividere la procedura in alcuni passi:
\begin{itemize}[leftmargin=1em]
    \item \textbf{\textit{Passo 1}: ottenimento della libreria \texttt{ptlcall.h}} \\
    Subito dopo essere entrati nel container è necessario lanciare un comando per copiare il file \texttt{ptcalls.h} all'interno della cartella "disks".
    \begin{lstlisting}[style=mystyle, language= C]
root@{...}:~/marss# cp /root/marss/marss/ptlsim/tools/ptlcalls.h /root/marss/disks/
    \end{lstlisting}

    \vspace{8pt}
    \item \textbf{\textit{Passo 2}: modifica del codice C}\\
    Il codice C rimarrà simile a quello utilizzato per l'esecuzione del workload con Gem5: verranno aggiunte solamente delle righe in più per poter aggiungere le librerie descritte prima. Per fare ciò, si utilizza il seguente comando:
    \begin{lstlisting}[style=mystyle, language= C]
root@{...}:~/marss# cd marss/disks
    \end{lstlisting}
    \begin{lstlisting}[style=mystyle, language= C]
root@{...}:~/marss/marss/disks# cat > matrix_m.c <<EOF
#include <stdio.h>
#include <stdlib.h>
#include "ptlcalls.h" // La libreria per il controllo ROI

#define N 64 

void matrix_multiply(double *A, double *B, double *C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            double sum = 0.0;
            for (int k = 0; k < size; k++) {
                sum += A[i * size + k] * B[k * size + j];
            }
            C[i * size + j] = sum;
        }
    }
}

int main() {
    // Usiamo static invece di malloc per evitare crash nell'ambiente statico
    // La dimensione e' N*N * sizeof(double), gestita automaticamente
    static double A[N * N];
    static double B[N * N];
    static double C[N * N];

    printf("Inizializzazione dati (Veloce)...\n");

    // Inizializzazione deterinistica
    for (int i = 0; i < N * N; i++) {
        A[i] = 1.0;
        B[i] = 2.0;
    }

    printf("--- ATTIVO LA SIMULAZIONE ORA ---\n");
    printf("(Il computer rallentera' drasticamente per simulare i calcoli floating point)\n");

    // === START ROI ===
    // Qui inizia la misurazione precisa
    ptlcall_switch_to_sim();

    matrix_multiply(A, B, C, N);

    // === STOP ROI ===
    // Qui finisce la misurazione
    ptlcall_switch_to_native();

    printf("--- SIMULAZIONE TERMINATA ---\n");
    printf("Benchmark Completato! Risultato cella 0: %f\n", C[0]);

    return 0;
}
EOF
    \end{lstlisting}

    \vspace{8pt}
    \item \textbf{\textit{Passo 3}: compilazione del file C}\\
    Per effettuare la compilazione di \texttt{matrix\_mul.c}, eseguire i seguenti comandi uno alla volta:
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# rm matrix_mul 2>/dev/null
    \end{lstlisting}
    Prima di installare il compilatore per i file C è necessario installare alcune librerie:
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# apt-get update
    \end{lstlisting}
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# apt-get install musl-tools musl-dev    
    \end{lstlisting}
    Non appena le librerie sono state installate, far partire il comando di compilazione:
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# gcc -std=gnu99 -D_GNU_SOURCE -static -nostartfiles -nostdinc \
     -I . \
     -I /usr/include/x86_64-linux-musl \
     -L /usr/lib/x86_64-linux-musl \
     /usr/lib/x86_64-linux-musl/crt1.o \
     /usr/lib/x86_64-linux-musl/crti.o \
     matrix_mul.c \
     /usr/lib/x86_64-linux-musl/crtn.o \
    -lc -o matrix_mul  
    \end{lstlisting}
    Poiché l'ambiente virtualizzato (Guest) è isolato dal sistema ospitante (Host), è necessario creare un meccanismo di trasferimento per l'eseguibile compilato.\\
    Quindi, tramite l'utility genisoimage, il file binario del benchmark (file C) viene incapsulato in un'immagine disco standard (ISO 9660). Questa immagine viene successivamente montata come unità CD-ROM virtuale all'avvio di QEMU, permettendo al sistema operativo Debian di leggere e copiare il file al suo interno.
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# rm trasferimento.iso
    \end{lstlisting}
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss/disks# genisoimage -o trasferimento.iso matrix_mul
    \end{lstlisting}

    \vspace{8pt}
    \item \textbf{\textit{Passo 4}: configurazione delle CPU e delle Cache}\\
    L'ultimo passaggio prima di procedere con l'esecuzione del workload è leggermente macchinoso. Infatti con il simulatore MarssX86 non è possibile specificare le tipologie di CPU o di Cache direttamente nel comando di build del workload (come si è fatto con Gem5) ma è necessario configurare alcuni file.

    \vspace{8pt}
    \noindent
    \textbf{\textit{Scelta della CPU}}\\
    Posizionati in \texttt{/root/marss/marss}, si andrà a creare una cartella \textit{"my\_configs"} all'interno della quale si inseriranno dei file che, oltre a specificare \textbf{il tipo di CPU} che si vuole utilizzare in base alla configurazione che si vuole buildare, presenta anche ulteriori comandi per \textbf{indicare alla distribuzione che file creare per salvare le informazioni importanti} relative al workload eseguito.\\
    Proprio per questo motivo si rende necessario creare un ulteriore cartella dove salvare i file delle statistiche:
    \begin{lstlisting}[style=mystyle, language=C]
root@{...}:~/marss/marss# mkdir results
    \end{lstlisting}

    \vspace{8pt}
    \noindent
    Ogni file presenta una configurazione di questo tipo:
        \begin{lstlisting}[style=mystyle, language=C]
-machine <tipo_di_cpu>
-logfile results/<nome_configurazione>.log 
-stats results/cache_<nome_configurazione>.txt 
-loglevel 2 
        \end{lstlisting}
    All'interno di \texttt{<nome\_configurazione>.log} vengono inserite le informazioni riguardanti cicli per istruzione (CPI), istruzioni per ciclo (IPC), tempo di simulazione, cicli di cpu e numero istruzioni, ecc\dots, invece, all'interno di \texttt{cache\_<nome\_configurazione>.txt}, vengono inserite, tra le altre, le informazioni riguardanti i Cache hit e i Cache miss di L1 e L2.

    \vspace{8pt}
    \noindent
    \begin{itemize}[leftmargin=1em, label=\raisebox{0.4ex}{\phantom{2}\rule{0.6ex}{0.6ex}}]
        \item \textbf{\textit{baseline.cfg}}: viene utilizzato per la configurazione baseline. All'interno di esso, è specificata una CPU di tipo out-of-order, scrivendo (\texttt{-machine single\_core});
        \begin{lstlisting}[style=mystyle, language=C]
-machine single_core
-logfile results/baseline.log 
-stats results/stats_baseline.txt 
-loglevel 2
        \end{lstlisting}
        \item \textbf{\textit{atom\_core.cfg}}: viene utilizzato per la configurazione CPU efficency test. All'interno di esso è specificata una CPU in-order, scrivendo (\texttt{-machine atom\_core});
        \begin{lstlisting}[style=mystyle, language=C]
-machine atom_core
-logfile results/atom_core.log
-stats results/cache_atom.txt
-loglevel 2
        \end{lstlisting}
        \item \textbf{\textit{small\_cache.cfg}}: viene utilizzato per la configurazione memory constraints test.\\ All'interno di esso viene specificata la stessa CPU out-of-order della baseline poiché in questo caso ciò che cambia è la dimensione delle Cache.
        \begin{lstlisting}[style=mystyle, language=C]
-machine single_core
-logfile results/small_caches.log
-stats results/cache_smallC.txt
-loglevel 2
        \end{lstlisting}
    \end{itemize}
    \vspace{8pt}
    \noindent 
    \textbf{\textit{Note}}: se non viene resa possibile la creazione dei file tramite interfaccia grafica è necessario crearli direttamente da riga di comando utilizzando il comando "sudo".

    \vspace{8pt}
    \noindent
    \textbf{\textit{Scelta delle Cache L1 ed L2}}\\
    All'avvio di QEMU, le Cache L1 e L2 vengono preimpostate rispettivamente a 128KB e 2MB.\\
    Ciò significa che, dopo la build della prima e della seconda configurazione (entrambe con gli stessi valori di Cache L1 ed L2), per utilizzare delle delle Cache più piccole per la terza configurazione, sarà necesario ricompilare il simulatore prima dell'avvio di QEMU.\\
    Per fare ciò vengono utilizzati i seguenti comandi (posizionati in \texttt{root/marss/marss})
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss# sed -i 's/SIZE: 128K/SIZE: 32K/g' config/l1_cache.conf
root@{...}:~/marss/marss# sed -i 's/SIZE: 2M/SIZE: 256K/g' config/l2_cache.conf
    \end{lstlisting}
    Successivamente è necessario pulire la vecchia compilazione per forzare la creazione del nuovo "binario" di compilazione, tramite il comando:
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss# scons -c
    \end{lstlisting}
    Infine, si rieffettua la compilazione per apportare le modifiche sulle Cache:
    \begin{lstlisting}[style=mystyle]
root@{...}:~/marss/marss# scons -Q C=1 debug=0
    \end{lstlisting}
\end{itemize}

\subsubsection{Comandi per la build del workload} 
Dopo aver configurato i file per le CPU e capito come impostare le Cache, si può finalmente passare alle build del workload.

\vspace{8pt}
\noindent
\textbf{\textit{Configurazione 1: baseline}}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 1 (baseline)
root@{...}:~/marss/marss# ./qemu/qemu-system-x86_64 -m 1G \
 -hda /root/marss/disks/debian_old.qcow2 \
 -cdrom /root/marss/disks/trasferimento.iso \
 -nographic \
 -simconfig my_configs/baseline.cfg
\end{lstlisting}
Successivamente verranno chieste le credenziali per l'accesso all'interno di debian (username: root, password root) e si entrerà all'interno della distribuzione. Una volta dentro si monta il CD e si copia il file:
\begin{lstlisting}[style=mystyle, language= C]
root@debian-amd64:~# mount /dev/cdrom /mnt
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
root@debian-amd64:~# cp /mnt/matrix_m .
\end{lstlisting}
Si danno i permessi di esecuzione, e poi si lancia:
\begin{lstlisting}[style=mystyle, language= C]
root@debian-amd64:~# chmod +x matrix_m
\end{lstlisting}
\begin{lstlisting}[style=mystyle, language= C]
root@debian-amd64:~# ./matrix_m
\end{lstlisting}
Arrivati a questo punto la build del workload è terminata e si può procedere con l'uscire dalla distribuzione e tornare al container tramite il seguente comando:
\begin{lstlisting}[style=mystyle, language= C]
root@debian-amd64:~# poweroff
\end{lstlisting}
Le statistiche della build verrano salvate all'interno dei file specificati al {\textit{"passo 4"}} in \textbf{"Scelta della CPU"} del capitolo \ref{par: ch_2.6.1}.

\vspace{8pt}
\noindent
\textbf{\textit{Configurazione 2: CPU efficiency test}}
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 2 (CPU efficiency test) 
root@{...}:~/marss/marss# ./qemu/qemu-system-x86_64 -m 1G \
 -hda /root/marss/disks/debian_old.qcow2 \
 -cdrom /root/marss/disks/trasferimento.iso \
 -nographic \
 -simconfig my_configs/atom_core.cfg
\end{lstlisting}
Dopo aver fatto partire la distribuzione con la CPU specificata, il procedimento da seguire per l'esecuzione del workload è il medesimo della \textbf{\textit{Configurazione baseline}}.

\vspace{8pt}
\noindent
\textbf{\textit{Configurazione 3: Memory costraint test}}\\
In questo caso, dato che si vogliono utilizzare dei valori di Cache L1 e L2 differenti, prima di far partire la distribuzione con il comando sotto specificato, è necessario seguire il procedimento descritto al \textbf{\textit{"passo 4"}} in \textbf{"Scelta delle Cache L1 e L2"} del capitolo \ref{par: ch_2.6.1}.
\begin{lstlisting}[style=mystyle, language= C]
// Comando di build per la configurazione 3 (Memory costraint test)
root@{...}:~/marss/marss# ./qemu/qemu-system-x86_64 -m 1G \
 -hda /root/marss/disks/debian_old.qcow2 \
 -cdrom /root/marss/disks/trasferimento.iso \
 -nographic \
 -simconfig my_configs/small_cache.cfg
\end{lstlisting}
Dopo aver fatto partire la distribuzione con la CPU specificata, il procedimento da seguire per l'esecuzione del workload è il medesimo della \textbf{\textit{Configurazione baseline}}. 




\end{document}